# ğŸ”§ Paperspace Training Fix Guide

**Date**: 2025-10-06
**Issue**: GPU Conflict + Poor Model Performance
**Status**: Ready to Fix

---

## ğŸ¯ à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆà¹€à¸ˆà¸­à¹ƒà¸™ Paperspace:

1. âŒ `CatBoostError: device already requested 0`
2. âŒ XGBoost RÂ² = -0.06 (à¹à¸¢à¹ˆà¸¡à¸²à¸)
3. âŒ LightGBM RÂ² = -0.86 (à¹à¸¢à¹ˆà¸¡à¸²à¸)
4. âŒ Training à¹€à¸ªà¸£à¹‡à¸ˆà¹€à¸£à¹‡à¸§à¹€à¸à¸´à¸™à¹„à¸› (1.5 à¸Šà¸¡. à¹à¸—à¸™ 9-12 à¸Šà¸¡.)

**à¸ªà¸²à¹€à¸«à¸•à¸¸**: Code à¹ƒà¸™ Paperspace outdated - à¹„à¸¡à¹ˆà¸¡à¸µ Session 011C à¹à¸¥à¸° 011D fixes

---

## âœ… Solution: Pull Latest Code from GitHub

### Step 1: Open Paperspace Terminal

à¹ƒà¸™ Paperspace Jupyter Notebook:
1. à¸„à¸¥à¸´à¸ **"+"** (New Launcher)
2. à¹€à¸¥à¸·à¸­à¸ **"Terminal"**

---

### Step 2: Pull Latest Fixes

à¸£à¸±à¸™à¸„à¸³à¸ªà¸±à¹ˆà¸‡à¸™à¸µà¹‰à¹ƒà¸™ Terminal:

```bash
# Navigate to project directory
cd /storage/ML-number

# Pull latest fixes from GitHub
git pull origin main

# Should see:
# Updating ef12477..28fe4c0
# Fast-forward
#  src/model_utils.py | 8 ++++----
#  src/evaluate.py    | 2 +-
#  ...
```

**Expected Output:**
```
From https://github.com/Useforclaude/ML-number
 * branch            main       -> FETCH_HEAD
Updating XXXXXXX..28fe4c0
Fast-forward
 src/model_utils.py              | 8 ++++----
 src/evaluate.py                 | 2 +-
 SESSION_011C_GPU_FIX.md         | 238 +++++++++++++++++++++++
 SESSION_011D_SKLEARN17_FIX.md   | 312 ++++++++++++++++++++++++++++
 NEXT_SESSION.md                 | 35 +++-
 ...
```

---

### Step 3: Verify Fixes Applied

à¸£à¸±à¸™à¸„à¸³à¸ªà¸±à¹ˆà¸‡à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰à¹€à¸à¸·à¹ˆà¸­à¸¢à¸·à¸™à¸¢à¸±à¸™:

```bash
# Check Session 011C fix (n_jobs=1)
echo "=== Checking GPU Conflict Fix (Session 011C) ==="
grep -n "n_jobs=1.*Prevent GPU" src/model_utils.py

# Expected output:
# 825:            n_jobs=1,  # Prevent GPU device conflict with CatBoost/XGBoost

# Check Session 011D fix (params)
echo ""
echo "=== Checking sklearn 1.7 Fix (Session 011D) ==="
grep -n "params={'sample_weight'" src/model_utils.py | head -3

# Expected output:
# 443:                params={'sample_weight': sample_weight},  # sklearn 1.7+
# 595:                params={'sample_weight': sample_weight},  # sklearn 1.7+
# 701:                params={'sample_weight': sample_weight},  # sklearn 1.7+

# Check git commits
echo ""
echo "=== Latest Git Commits ==="
git log --oneline -5

# Expected output:
# 28fe4c0 Add Session 011D documentation
# b626090 Session 011D: Fix scikit-learn 1.7 API
# 70189de Add SESSION_011C_GPU_FIX.md
# 07edf24 Update NEXT_SESSION.md
# ef12477 Session 011C: Fix CatBoost GPU conflict
```

---

### Step 4: Restart Jupyter Kernel

1. à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸µà¹ˆ Jupyter Notebook tab
2. à¸„à¸¥à¸´à¸ menu **"Kernel"**
3. à¹€à¸¥à¸·à¸­à¸ **"Restart & Clear Output"**
4. à¸„à¸¥à¸´à¸ **"Restart"** à¸¢à¸·à¸™à¸¢à¸±à¸™

---

### Step 5: Re-run All Cells

**Cell 1: Install Libraries** (à¸–à¹‰à¸²à¸¡à¸µ)
```python
!pip install -q xgboost lightgbm catboost optuna
```

**Cell 2: Setup Environment**
```python
import sys
sys.path.insert(0, '/storage/ML-number')

from src.config import BASE_PATH
print(f"âœ… BASE_PATH: {BASE_PATH}")
```

**Cell 3: Load Data & Features**
```python
# Your data loading code here
```

**Cell 4: Training**
```python
from src.train_production import train_production_pipeline

# IMPORTANT: Verify these parameters
print(f"ğŸ¯ optimize = True")
print(f"ğŸ¯ n_trials = 100")
print(f"ğŸ¯ use_gpu = True")

result = train_production_pipeline(
    X_train, y_train,
    X_test, y_test,
    optimize=True,        # âœ… Must be True
    n_trials=100,         # âœ… Must be 100 (not 0!)
    use_gpu=True,         # âœ… Use Paperspace GPU
    verbose=True
)
```

---

## ğŸ“Š Expected Results (After Fixes)

### Training Timeline:
```
â±ï¸  XGBoost optimization:    ~2.5 hours (100 trials)
â±ï¸  LightGBM optimization:   ~3.5 hours (100 trials)
â±ï¸  CatBoost optimization:   ~1.5 hours (100 trials)
â±ï¸  RandomForest optimization: ~1.0 hour (100 trials)
â±ï¸  Ensemble creation:       ~0.5 hours
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â±ï¸  Total:                   ~9-12 hours
```

**âš ï¸ à¸–à¹‰à¸²à¸£à¸±à¸™à¹€à¸ªà¸£à¹‡à¸ˆà¸ à¸²à¸¢à¹ƒà¸™ < 2 à¸Šà¸¡. = optimization à¹„à¸¡à¹ˆà¸—à¸³à¸‡à¸²à¸™!**

---

### Model Performance:
```
Individual Models (with optimization):
âœ… GradientBoosting:  RÂ² > 0.90
âœ… CatBoost:          RÂ² > 0.88
âœ… RandomForest:      RÂ² > 0.85
âœ… XGBoost:           RÂ² > 0.87  (not -0.06!)
âœ… LightGBM:          RÂ² > 0.86  (not -0.86!)
âœ… ExtraTrees:        RÂ² > 0.82

Ensemble Models:
âœ… Stacking Ensemble:  RÂ² > 0.93  (best model)
âœ… Voting Ensemble:    RÂ² > 0.91
âœ… Weighted Average:   RÂ² > 0.90
```

---

### Optuna Progress (Must See This!):
```
================================================================================
âšª Training XGBoost (GPU)
================================================================================
ğŸ¯ Target: Find best hyperparameters
ğŸ”¢ Trials: 100
ğŸ“Š Method: Optuna TPE Sampler + 10-Fold CV
â±ï¸  Started: 2025-10-06 21:30:00
================================================================================

[I 2025-10-06 21:30:01] Trial 0 complete. Value: 0.8523
[I 2025-10-06 21:30:15] Trial 1 complete. Value: 0.8612
[I 2025-10-06 21:30:29] Trial 2 complete. Value: 0.8701
...
[I 2025-10-06 23:15:42] Trial 99 complete. Value: 0.9234

âœ… Best RÂ²: 0.9234
â±ï¸  Time: 1.75 hours
```

**à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹€à¸«à¹‡à¸™ log à¸™à¸µà¹‰ = optimization à¹„à¸¡à¹ˆà¸£à¸±à¸™!**

---

## ğŸš¨ Troubleshooting

### Issue 1: "Already up to date" à¹à¸•à¹ˆ code à¸¢à¸±à¸‡à¹€à¸à¹ˆà¸²

```bash
# Force pull
cd /storage/ML-number
git fetch origin
git reset --hard origin/main

# Verify again
git log --oneline -5
```

---

### Issue 2: Training à¸¢à¸±à¸‡à¹€à¸£à¹‡à¸§à¹€à¸à¸´à¸™à¹„à¸› (< 2 à¸Šà¸¡.)

**Check 1: Verify n_trials in Cell 4**
```python
# MUST be 100, NOT 0
n_trials = 100
```

**Check 2: Check if optimize=True actually works**
```python
# Add debug print
print(f"DEBUG: optimize={optimize}, n_trials={n_trials}")

# Before calling train_production_pipeline
```

**Check 3: Check train_production.py**
```bash
# Verify optimize parameter is used
grep -n "if optimize:" src/train_production.py
```

---

### Issue 3: GPU Conflict à¸¢à¸±à¸‡à¹€à¸à¸´à¸”

```bash
# Verify n_jobs=1 fix exists
grep -n "n_jobs=1" src/model_utils.py src/train.py

# Should see:
# src/model_utils.py:825:            n_jobs=1,  # Prevent GPU device conflict
# src/train.py:338:voting_ensemble = VotingRegressor(voting_models, n_jobs=1)
```

---

### Issue 4: fit_params error à¸¢à¸±à¸‡à¹€à¸à¸´à¸”

```bash
# Verify params (not fit_params) is used
grep -n "fit_params" src/model_utils.py src/evaluate.py

# Should return: (no results)

# Check params is used instead
grep -n "params={'sample_weight'" src/model_utils.py

# Should see 4 results (XGBoost, LightGBM, CatBoost, RF)
```

---

## âœ… Success Checklist

- [ ] `git pull` à¹€à¸ªà¸£à¹‡à¸ˆà¹„à¸¡à¹ˆà¸¡à¸µ error
- [ ] `grep n_jobs=1` à¹€à¸ˆà¸­à¹ƒà¸™ model_utils.py
- [ ] `grep params=` à¹€à¸ˆà¸­à¹ƒà¸™ model_utils.py à¹à¸¥à¸° evaluate.py
- [ ] `git log` à¹à¸ªà¸”à¸‡ commit 28fe4c0, b626090, ef12477
- [ ] Kernel restart à¸ªà¸³à¹€à¸£à¹‡à¸ˆ
- [ ] Optuna trials à¹à¸ªà¸”à¸‡à¹ƒà¸™ output (Trial 0, 1, 2, ...)
- [ ] Training à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸² 9-12 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ (à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ 1.5 à¸Šà¸¡.)
- [ ] Individual model RÂ² > 0.85
- [ ] Ensemble model RÂ² > 0.90
- [ ] à¹„à¸¡à¹ˆà¸¡à¸µ GPU conflict error
- [ ] à¹„à¸¡à¹ˆà¸¡à¸µ fit_params error

---

## ğŸ“ à¸–à¹‰à¸²à¸¢à¸±à¸‡à¸¡à¸µà¸›à¸±à¸à¸«à¸²

1. à¹à¸ªà¸”à¸‡ output à¸‚à¸­à¸‡ `git pull`
2. à¹à¸ªà¸”à¸‡ output à¸‚à¸­à¸‡ `git log --oneline -5`
3. à¹à¸ªà¸”à¸‡ output à¸‚à¸­à¸‡ `grep n_jobs=1 src/model_utils.py`
4. à¹à¸ªà¸”à¸‡ training time à¹à¸¥à¸° RÂ² results
5. Copy error traceback (à¸–à¹‰à¸²à¸¡à¸µ)

---

**Created**: 2025-10-06
**Fixes Required**: Session 011C (GPU) + Session 011D (sklearn 1.7)
**Expected Training Time**: 9-12 hours
**Expected Best RÂ²**: > 0.93
