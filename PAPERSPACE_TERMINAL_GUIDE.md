# üöÄ Paperspace Terminal Guide - ML Phone Number Training

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö**: Train ML models ‡∏ö‡∏ô Paperspace ‡∏î‡πâ‡∏ß‡∏¢ Terminal (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Notebook)
**GPU**: Free Tier (M4000) ‡∏´‡∏£‡∏∑‡∏≠ Growth Plan (RTX A4000, P5000)
**‡∏£‡∏∞‡∏î‡∏±‡∏ö**: ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå
**‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô**: 1.0 - Initial release (2025-10-08)

---

## üìã ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°

**‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏û‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö:**

```
1. Clone ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡∏à‡∏≤‡∏Å GitHub (Terminal)      ‚Üí 2 ‡∏ô‡∏≤‡∏ó‡∏µ
2. Setup Environment + Install Deps (Terminal) ‚Üí 3-5 ‡∏ô‡∏≤‡∏ó‡∏µ
3. Upload/Prepare Data (Jupyter UI/Terminal)   ‚Üí 2 ‡∏ô‡∏≤‡∏ó‡∏µ
4. ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Script (Jupyter UI)         ‚Üí 3 ‡∏ô‡∏≤‡∏ó‡∏µ
5. ‡∏£‡∏±‡∏ô Training (Terminal - Background)        ‚Üí 9-12 ‡∏ä‡∏°. ‚ö°
6. Monitor Progress (Terminal)                 ‚Üí ‡∏ó‡∏∏‡∏Å 30-60 ‡∏ô‡∏≤‡∏ó‡∏µ
7. Download Results (Jupyter UI)               ‚Üí 2 ‡∏ô‡∏≤‡∏ó‡∏µ
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‡∏£‡∏ß‡∏°: ~10-15 ‡∏ô‡∏≤‡∏ó‡∏µ setup + 9-12 ‡∏ä‡∏°. training
```

**üí° ‡πÄ‡∏ß‡∏•‡∏≤ Training ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö GPU:**
- **RTX A4000**: 9-12 ‡∏ä‡∏°. (N_TRIALS=100)
- **RTX P5000**: 7-9 ‡∏ä‡∏°. (N_TRIALS=100)
- **M4000** (Free): 12-15 ‡∏ä‡∏°. (N_TRIALS=100)

**üîë ‡∏à‡∏∏‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á Terminal Method:**
- ‚úÖ ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏î‡πâ (training ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠)
- ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ idle timeout (‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏ô‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î)
- ‚úÖ Monitor progress ‡∏ï‡∏≠‡∏ô‡πÑ‡∏´‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ
- ‚úÖ Production-ready (‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á)

---

## ü§î Terminal vs Notebook - ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏¢‡∏±‡∏á‡πÑ‡∏á?

### üìä Paperspace Jupyter Lab ‡∏°‡∏µ 3 ‡∏™‡πà‡∏ß‡∏ô

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Paperspace Jupyter Lab              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ  1. Jupyter UI (File Browser)          ‚îÇ
‚îÇ     ‚Üí Upload/Download files            ‚îÇ
‚îÇ     ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á/‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå                     ‚îÇ
‚îÇ     ‚Üí ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Notebook!                 ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  2. Terminal (Command Line)             ‚îÇ
‚îÇ     ‚Üí ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á, git, python           ‚îÇ
‚îÇ     ‚Üí ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å! ‚≠ê          ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  3. Notebook (.ipynb) [‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ]‚îÇ
‚îÇ     ‚Üí Interactive cells                ‚îÇ
‚îÇ     ‚Üí ‡∏°‡∏µ idle timeout                  ‚îÇ
‚îÇ                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### ‚úÖ ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á?

| PART | ‡πÉ‡∏ä‡πâ‡∏≠‡∏∞‡πÑ‡∏£ | ‡∏ó‡∏≥‡∏≠‡∏∞‡πÑ‡∏£ |
|------|---------|--------|
| **PART 1** | **Terminal** | Clone GitHub repo |
| **PART 2** | **Terminal** | Setup venv + install dependencies |
| **PART 3** | **Jupyter UI** | Upload numberdata.csv (optional) |
| **PART 4** | **Jupyter UI** | ‡∏™‡∏£‡πâ‡∏≤‡∏á training script (.py) |
| **PART 5** | **Terminal** | ‡∏£‡∏±‡∏ô training (background) |
| **PART 6** | **Terminal** | Monitor progress |
| **PART 7** | **Jupyter UI** | Download models + results |

**üéØ ‡∏™‡∏£‡∏∏‡∏õ:**
- ‚úÖ **Terminal** - ‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏´‡∏•‡∏±‡∏Å!)
- ‚úÖ **Jupyter UI** - Upload/Download files
- ‚ùå **Notebook (.ipynb)** - ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏¢!

---

### üîç ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á Terminal vs Notebook

| | **Terminal** | **Notebook (.ipynb)** |
|---|--------------|---------------------|
| **‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö** | Command line | Interactive cells |
| **‡∏£‡∏±‡∏ô‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á** | `python train.py` | Cell-by-cell |
| **‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö** | Long tasks (9-12 ‡∏ä‡∏°.) | Quick experiments |
| **Timeout** | ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ | ‚úÖ ‡∏°‡∏µ (idle ‚Üí disconnect) |
| **‡∏õ‡∏¥‡∏î Browser** | ‚úÖ ‡πÑ‡∏î‡πâ (process ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô) | ‚ùå ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ (kernel ‡∏ï‡∏≤‡∏¢) |
| **GPU** | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏ï‡πá‡∏°‡∏ó‡∏µ‡πà |
| **‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û** | **‚ö°‚ö°‚ö°‚ö°‚ö°** | **‚ö°‚ö°‚ö°‚ö°‚ö°** (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô!) |
| **‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠** | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å | ‚ùå ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ |

---

### ‚ö° ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û - ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?

```
‚úÖ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á!

‡∏ó‡∏±‡πâ‡∏á Terminal ‡πÅ‡∏•‡∏∞ Notebook ‡πÉ‡∏ä‡πâ:
- GPU ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (RTX A4000)
- RAM ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
- CPU ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
- VRAM ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô

‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô performance ‡πÄ‡∏•‡∏¢!
```

**üîç ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
- Terminal: ‡∏£‡∏±‡∏ô `python train.py` ‚Üí ‡πÉ‡∏ä‡πâ Python interpreter
- Notebook: ‡∏£‡∏±‡∏ô cell ‚Üí ‡πÉ‡∏ä‡πâ Python interpreter **‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô**
- ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á GPU ‡πÑ‡∏î‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:**
```bash
# Terminal
python train_terminal.py
‚Üí ‡πÉ‡∏ä‡πâ GPU RTX A4000 ‚úì

# Notebook cell
!python train_terminal.py
‚Üí ‡πÉ‡∏ä‡πâ GPU RTX A4000 ‚úì  (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô!)
```

---

### ü§î ‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡∏ó‡∏≥‡πÑ‡∏° Notebook ‡∏ñ‡∏∂‡∏á Timeout?

**‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û (Performance) ‚â† Connection Management**

**‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Terminal Process                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  Browser ‚Üí Paperspace Server               ‚îÇ
‚îÇ              ‚Üì                              ‚îÇ
‚îÇ         Terminal (bash)                     ‚îÇ
‚îÇ              ‚Üì                              ‚îÇ
‚îÇ    python train.py (‡∏£‡∏±‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞)              ‚îÇ
‚îÇ              ‚Üì                              ‚îÇ
‚îÇ            GPU ‚ö°                            ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚úÖ ‡∏õ‡∏¥‡∏î browser ‚Üí process ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠!      ‚îÇ
‚îÇ  ‚úÖ Network ‡∏Ç‡∏≤‡∏î ‚Üí process ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠!       ‚îÇ
‚îÇ  ‚úÖ Idle 10 ‡∏ä‡∏°. ‚Üí ‡πÑ‡∏°‡πà‡∏°‡∏µ timeout!           ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Notebook Cell Process               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  Browser ‚Üî Jupyter Kernel ‚Üî Python Process ‚îÇ
‚îÇ     ‚Üï          ‚Üï              ‚Üï             ‚îÇ
‚îÇ  Activity   Heartbeat        GPU ‚ö°          ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚ùå ‡∏õ‡∏¥‡∏î browser ‚Üí kernel ‡∏ï‡∏≤‡∏¢ ‚Üí process ‡∏´‡∏¢‡∏∏‡∏î!‚îÇ
‚îÇ  ‚ùå Network ‡∏Ç‡∏≤‡∏î ‚Üí kernel ‡∏ï‡∏≤‡∏¢ ‚Üí process ‡∏´‡∏¢‡∏∏‡∏î!‚îÇ
‚îÇ  ‚ùå Idle ‡∏ô‡∏≤‡∏ô ‚Üí kernel timeout ‚Üí process ‡∏´‡∏¢‡∏∏‡∏î!‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### üí° ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢:

**Terminal = ‡∏™‡πà‡∏á‡∏û‡∏±‡∏™‡∏î‡∏∏ (Drop & Go)**
```
‡∏Ñ‡∏∏‡∏ì‡∏™‡πà‡∏á‡∏á‡∏≤‡∏ô (python train.py) ‚Üí Paperspace ‡∏£‡∏±‡∏ö‡πÑ‡∏ß‡πâ ‚Üí ‡∏Ñ‡∏∏‡∏ì‡∏õ‡∏¥‡∏î browser
‚Üí ‡∏á‡∏≤‡∏ô‡∏ó‡∏≥‡∏ï‡πà‡∏≠‡πÄ‡∏≠‡∏á (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°)
‚Üí ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô ‚úì
```

**Notebook = ‡πÅ‡∏ó‡πá‡∏Å‡∏ã‡∏µ‡πà (Stay Connected)**
```
‡∏Ñ‡∏∏‡∏ì‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô ‚Üí ‡∏Ç‡∏ì‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô...
‚Üí ‡∏Ñ‡∏∏‡∏ì‡∏õ‡∏¥‡∏î browser ‚Üí Jupyter ‡∏Ñ‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ô ‚Üí ‡∏´‡∏¢‡∏∏‡∏î‡∏á‡∏≤‡∏ô ‚úó
‚Üí ‡∏Ñ‡∏∏‡∏ì idle ‡∏ô‡∏≤‡∏ô ‚Üí Jupyter timeout ‚Üí ‡∏´‡∏¢‡∏∏‡∏î‡∏á‡∏≤‡∏ô ‚úó
‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏•‡∏ï‡∏•‡∏≠‡∏î (‡∏°‡∏µ activity) ‚Üí ‡πÄ‡∏™‡∏£‡πá‡∏à ‚úì
```

---

### üéØ ‡∏ó‡∏≥‡πÑ‡∏°‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ Terminal?

**1. ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Long-running Tasks**
```
ML Training 9-12 ‡∏ä‡∏°. ‚Üí ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô
‚Üí Terminal: ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‚úì
‚Üí Notebook: ‡∏≠‡∏≤‡∏à timeout (‡∏ñ‡πâ‡∏≤ idle) ‚úó
```

**2. Production-ready**
```
Terminal script ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ:
- Deploy ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
- ‡∏£‡∏±‡∏ô‡πÉ‡∏ô CI/CD
- Automate ‡πÑ‡∏î‡πâ
- Scale ‡πÑ‡∏î‡πâ
```

**3. ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏á‡∏ß‡∏•**
```
‚Üí ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏î‡πâ
‚Üí ‡πÑ‡∏õ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô
‚Üí ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ‡∏ï‡∏≠‡∏ô‡πÑ‡∏´‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ
‚Üí ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏¥‡∏î‡∏Ñ‡πâ‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏ß‡∏±‡∏ô
```

---

## üåç Platform Comparison

| Platform | Terminal/SSH | ‡∏õ‡∏¥‡∏î Browser ‡πÑ‡∏î‡πâ | Idle Timeout | GPU ‡∏ü‡∏£‡∏µ |
|----------|--------------|-----------------|--------------|---------|
| **Paperspace** | ‚úÖ ‡∏°‡∏µ | ‚úÖ ‡πÑ‡∏î‡πâ | ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ | ‚úÖ ‡∏°‡∏µ (Free) |
| **AWS/GCP** | ‚úÖ ‡∏°‡∏µ (SSH) | ‚úÖ ‡πÑ‡∏î‡πâ | ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ | ‚ùå ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô |
| **RunPod** | ‚úÖ ‡∏°‡∏µ (SSH) | ‚úÖ ‡πÑ‡∏î‡πâ | ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ | ‚ùå ‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏á‡∏¥‡∏ô |
| **Google Colab** | ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ | ‚ùå ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ | ‚úÖ ‡∏°‡∏µ (90 min) | ‚úÖ ‡∏°‡∏µ (‡∏à‡∏≥‡∏Å‡∏±‡∏î) |
| **Kaggle** | ‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ | ‚ùå ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ | ‚úÖ ‡∏°‡∏µ (9 hr) | ‚úÖ ‡∏°‡∏µ (P100) |

**üí° ‡∏Å‡∏é‡∏á‡πà‡∏≤‡∏¢‡πÜ:**
```
‡∏°‡∏µ Terminal access ‚Üí ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏î‡πâ ‚úÖ
‡πÑ‡∏°‡πà‡∏°‡∏µ (Colab, Kaggle) ‚Üí ‡∏õ‡∏¥‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‚ùå
```

---

## PART 1: Clone ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

**‚ö†Ô∏è ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡∏ó‡∏≥‡πÉ‡∏ô Paperspace Notebook Terminal (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì!)**

### ‚úÖ STEP 1: ‡πÄ‡∏õ‡∏¥‡∏î Terminal

**‡πÉ‡∏ô Jupyter Lab:**

```
1. ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á File Browser ‚Üí ‡∏°‡∏µ‡∏õ‡∏∏‡πà‡∏° "+" (New Launcher)
2. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° "+"
3. ‡πÉ‡∏ô Launcher ‚Üí ‡∏°‡∏≠‡∏á‡∏´‡∏≤ "Other" section
4. ‡∏Ñ‡∏•‡∏¥‡∏Å "Terminal"
5. Terminal ‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤ (‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏î‡∏≥‡πÜ ‡∏°‡∏µ prompt)
```

---

### ‚úÖ STEP 2: ‡πÄ‡∏ä‡πá‡∏Ñ GPU üî• **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!**

**‚ö†Ô∏è ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î! ‡∏ñ‡πâ‡∏≤ GPU ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‚Üí Training ‡∏à‡∏∞‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å!**

**‡πÉ‡∏ô Terminal - Copy ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡πâ‡∏ß Paste:**

```bash
nvidia-smi
```

**‡∏Å‡∏î Enter**

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI xxx.xx       Driver Version: xxx.xx       CUDA Version: xx.x    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        ...          | ...                  | ...                  |
|   0  Quadro M4000       ...   | ...                  | ...                  |
+-------------------------------+----------------------+----------------------+
```

**‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• GPU ‚Üí ‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‚Üí ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ**

**‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô ‚Üí ‚ùå STOP! ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡∏Å‡πà‡∏≠‡∏ô:**
1. ‡∏õ‡∏¥‡∏î Notebook
2. Settings ‚Üí Machine ‚Üí ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å GPU machine
3. Start Notebook ‡πÉ‡∏´‡∏°‡πà
4. ‡∏£‡∏±‡∏ô nvidia-smi ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô GPU

**üí° Double-check: ‡πÄ‡∏ä‡πá‡∏Ñ PyTorch ‡πÄ‡∏´‡πá‡∏ô GPU ‡∏°‡∏±‡πâ‡∏¢**

```bash
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
CUDA available: True
```

---

### ‚úÖ STEP 3: Clone ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

**‡πÉ‡∏ô Terminal:**

```bash
# Navigate to persistent storage
cd /storage

# Clone GitHub repository
git clone https://github.com/Useforclaude/ML-number.git

# Enter project directory
cd ML-number

# Check files
ls -lh
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
total 500K
drwxr-xr-x 2 user user 4.0K Oct  8 10:00 data
drwxr-xr-x 2 user user 4.0K Oct  8 10:00 models
-rw-r--r-- 1 user user  15K Oct  8 10:00 NEXT_SESSION.md
-rw-r--r-- 1 user user  12K Oct  8 10:00 README.md
-rw-r--r-- 1 user user 1.5K Oct  8 10:00 requirements.txt
drwxr-xr-x 2 user user 4.0K Oct  8 10:00 src
...
```

‚úÖ **‡πÑ‡∏î‡πâ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÅ‡∏•‡πâ‡∏ß!**

---

## PART 2: Setup Environment

### ‚úÖ STEP 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á Virtual Environment

**‡πÉ‡∏ô Terminal (‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà /storage/ML-number):**

```bash
# Create virtual environment
python3 -m venv .venv

# Activate virtual environment
source .venv/bin/activate

# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô (.venv) ‡πÉ‡∏ô prompt:
# (.venv) user@paperspace:/storage/ML-number$
```

‚úÖ **Virtual environment ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!**

---

### ‚úÖ STEP 5: Install Dependencies

**‡πÉ‡∏ô Terminal (venv activated):**

```bash
# Upgrade pip
pip install --upgrade pip

# Install dependencies
pip install -r requirements.txt

# ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ 3-5 ‡∏ô‡∏≤‡∏ó‡∏µ
# ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô: Installing collected packages: numpy, pandas, scikit-learn, ...
```

**‚úÖ Dependencies ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏™‡∏£‡πá‡∏à!**

---

### ‚úÖ STEP 6: Verify Installation

**‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ imports ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**

```bash
python -c "from src.config import BASE_PATH; print(f'‚úÖ BASE_PATH: {BASE_PATH}')"
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
‚úÖ BASE_PATH: /storage/ML-number
```

**‡πÄ‡∏ä‡πá‡∏Ñ GPU support:**

```bash
python -c "import torch; print('CUDA:', torch.cuda.is_available()); import xgboost as xgb; print('XGBoost version:', xgb.__version__)"
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
CUDA: True
XGBoost version: 1.7.6
```

‚úÖ **Setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**

---

## PART 3: Prepare Data

### ‚úÖ STEP 7: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

**‡πÉ‡∏ô Terminal:**

```bash
# Check data file
ls -lh data/raw/

# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô numberdata.csv
```

**‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‚úÖ ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ PART 4**

**‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ‚Üí ‡∏ï‡πâ‡∏≠‡∏á upload:**

---

### ‚úÖ STEP 8: Upload Data (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)

**Option 1: ‡πÉ‡∏ä‡πâ Jupyter UI (‡∏á‡πà‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)**

```
1. ‡πÉ‡∏ô File Browser ‚Üí Navigate to ML-number/data/raw/
2. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏Ç‡∏ß‡∏≤‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Upload"
4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå numberdata.csv
5. ‡∏£‡∏≠‡∏à‡∏ô‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à
```

**Option 2: ‡πÉ‡∏ä‡πâ wget (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ URL)**

```bash
cd /storage/ML-number/data/raw/
wget https://your-url.com/numberdata.csv
```

**Option 3: Copy ‡∏à‡∏≤‡∏Å Downloads (‡∏ñ‡πâ‡∏≤ mount ‡πÅ‡∏•‡πâ‡∏ß)**

```bash
cp /path/to/numberdata.csv /storage/ML-number/data/raw/
```

---

### ‚úÖ STEP 9: Verify Data

```bash
cd /storage/ML-number
python -c "import pandas as pd; df = pd.read_csv('data/raw/numberdata.csv'); print(f'‚úÖ Data: {len(df)} rows, {df.shape[1]} columns')"
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
‚úÖ Data: 6112 rows, 2 columns
```

‚úÖ **Data ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!**

---

## PART 4: ‡∏™‡∏£‡πâ‡∏≤‡∏á Training Script

### ‚úÖ STEP 10: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Script

**‡πÉ‡∏ô Jupyter UI File Browser:**

```
1. Navigate ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ML-number/
2. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏Ç‡∏ß‡∏≤‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "New File"
4. ‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏∑‡πà‡∏≠: train_terminal.py
5. ‡∏Å‡∏î Enter
```

‚úÖ **‡πÑ‡∏î‡πâ‡πÑ‡∏ü‡∏•‡πå `train_terminal.py` ‡πÅ‡∏•‡πâ‡∏ß**

---

### ‚úÖ STEP 11: ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á Code

**‡πÉ‡∏ô File Browser:**

```
1. Double-click ‡πÑ‡∏ü‡∏•‡πå train_terminal.py
   ‚Üí ‡πÄ‡∏õ‡∏¥‡∏î Text Editor

2. Copy code ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ
   ‚Üí Paste ‡∏•‡∏á‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå

3. ‡∏Å‡∏î Ctrl+S (‡∏´‡∏£‡∏∑‡∏≠ Cmd+S ‡∏ö‡∏ô Mac) = Save
```

**Code ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á Copy:**

```python
#!/usr/bin/env python3
"""
Paperspace ML Training - Terminal Version
Train phone number price prediction models with GPU support
"""

import os
import sys
import time
import logging
from datetime import datetime
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/training_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Import project modules
try:
    from src.config import BASE_PATH, MODEL_CONFIG
    from src.environment import detect_environment, setup_base_path
    from src.data_handler import load_and_clean_data
    from src.features import create_all_features
    from src.data_splitter import split_data_stratified, create_validation_set
    from src.model_utils import AdvancedPreprocessor
    from src.train_production import train_production_pipeline
    import numpy as np
    import pandas as pd
    import torch
except ImportError as e:
    logger.error(f"Import error: {e}")
    logger.error("Make sure virtual environment is activated: source .venv/bin/activate")
    sys.exit(1)


def print_header():
    """Print training header"""
    print("\n" + "="*80)
    print("üöÄ PAPERSPACE ML TRAINING - TERMINAL MODE")
    print("="*80)
    print(f"üìÇ Project: ML Phone Number Price Prediction")
    print(f"üìç Path: {BASE_PATH}")
    print(f"üñ•Ô∏è  Environment: {detect_environment()}")
    print(f"üéØ Target: R¬≤ > 0.90")
    print("="*80 + "\n")


def check_gpu():
    """Check GPU availability"""
    logger.info("üîç Checking GPU...")

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"‚úÖ GPU Available: {gpu_name}")
        logger.info(f"   Memory: {gpu_memory:.1f} GB")
        return True
    else:
        logger.warning("‚ö†Ô∏è  No GPU detected - Training will use CPU (much slower!)")
        return False


def main():
    """Main training pipeline"""
    start_time = time.time()

    # Print header
    print_header()

    # Check GPU
    use_gpu = check_gpu()

    # Step 1: Load data
    logger.info("\n" + "="*80)
    logger.info("STEP 1: Loading Data")
    logger.info("="*80)

    try:
        df_cleaned = load_and_clean_data()
        logger.info(f"‚úÖ Data loaded: {len(df_cleaned)} rows")
    except Exception as e:
        logger.error(f"‚ùå Failed to load data: {e}")
        return 1

    # Step 2: Feature engineering
    logger.info("\n" + "="*80)
    logger.info("STEP 2: Feature Engineering")
    logger.info("="*80)

    try:
        X, y_log, sample_weights = create_all_features(df_cleaned)
        logger.info(f"‚úÖ Features created: {X.shape[1]} features, {X.shape[0]} samples")
    except Exception as e:
        logger.error(f"‚ùå Failed to create features: {e}")
        return 1

    # Step 3: Split data
    logger.info("\n" + "="*80)
    logger.info("STEP 3: Train/Test Split")
    logger.info("="*80)

    try:
        X_train, X_test, y_log_train, y_log_test, sw_train, sw_test = split_data_stratified(
            X, y_log, sample_weights,
            test_size=0.2,
            random_state=42
        )
        logger.info(f"‚úÖ Split complete: Train={len(X_train)}, Test={len(X_test)}")
    except Exception as e:
        logger.error(f"‚ùå Failed to split data: {e}")
        return 1

    # Step 4: Convert to actual prices
    logger.info("\n" + "="*80)
    logger.info("STEP 4: Converting to Actual Prices")
    logger.info("="*80)

    y_train = pd.Series(np.expm1(y_log_train))
    y_test = pd.Series(np.expm1(y_log_test))
    logger.info(f"‚úÖ Train prices: ‡∏ø{y_train.min():,.0f} - ‡∏ø{y_train.max():,.0f}")
    logger.info(f"‚úÖ Test prices: ‡∏ø{y_test.min():,.0f} - ‡∏ø{y_test.max():,.0f}")

    # Step 5: Create validation set
    logger.info("\n" + "="*80)
    logger.info("STEP 5: Creating Validation Set")
    logger.info("="*80)

    try:
        X_tr, X_val, y_tr, y_val, sw_tr, sw_val = create_validation_set(
            X_train, y_train, sw_train,
            val_size=0.15,
            random_state=42
        )
        logger.info(f"‚úÖ Validation set: Train={len(X_tr)}, Val={len(X_val)}")
    except Exception as e:
        logger.error(f"‚ùå Failed to create validation set: {e}")
        return 1

    # Step 6: Preprocessing
    logger.info("\n" + "="*80)
    logger.info("STEP 6: Preprocessing")
    logger.info("="*80)

    try:
        preprocessor = AdvancedPreprocessor()
        X_tr_processed = preprocessor.fit_transform(X_tr)
        X_val_processed = preprocessor.transform(X_val)
        X_test_processed = preprocessor.transform(X_test)

        # Clean NaN/Inf
        for df in [X_tr_processed, X_val_processed, X_test_processed]:
            df.replace([np.inf, -np.inf], np.nan, inplace=True)
            if hasattr(df, 'median'):
                df.fillna(df.median(), inplace=True)
            else:
                df.fillna(X_tr_processed.median(), inplace=True)

        logger.info(f"‚úÖ Preprocessed: {X_tr_processed.shape[1]} features")
    except Exception as e:
        logger.error(f"‚ùå Failed to preprocess: {e}")
        return 1

    # Step 7: PRODUCTION TRAINING
    logger.info("\n" + "="*80)
    logger.info("üî• STEP 7: PRODUCTION TRAINING")
    logger.info("="*80)
    logger.info(f"‚è±Ô∏è  Expected duration: 9-12 hours")
    logger.info(f"üéØ Optimization trials: {MODEL_CONFIG.get('optuna_trials', 100)}")
    logger.info(f"üî• GPU enabled: {use_gpu}")
    logger.info("="*80 + "\n")

    training_start = time.time()

    try:
        results = train_production_pipeline(
            X_tr_processed, y_tr,
            X_val_processed, y_val,
            optimize=True,
            n_trials=MODEL_CONFIG.get('optuna_trials', 100),
            use_gpu=use_gpu,
            verbose=True
        )

        training_time = (time.time() - training_start) / 3600

        # Final results
        logger.info("\n" + "="*80)
        logger.info("‚úÖ TRAINING COMPLETE!")
        logger.info("="*80)
        logger.info(f"‚è±Ô∏è  Training Time: {training_time:.2f} hours")
        logger.info(f"üèÜ Best Model: {results['best_model_name']}")
        logger.info(f"üìä Best R¬≤: {results['best_score']:.4f}")
        logger.info(f"üìâ MAE: {results['best_mae']:.2f}")
        logger.info(f"üìâ RMSE: {results['best_rmse']:.2f}")
        logger.info("="*80)

        # Total time
        total_time = (time.time() - start_time) / 3600
        logger.info(f"\n‚è±Ô∏è  Total Time: {total_time:.2f} hours")
        logger.info("‚úÖ All done! Models saved to models/ directory")

        return 0

    except Exception as e:
        logger.error(f"‚ùå Training failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    # Create logs directory
    os.makedirs("logs", exist_ok=True)

    # Run training
    exit_code = main()
    sys.exit(exit_code)
```

**‡∏Å‡∏î Save (Ctrl+S)**

‚úÖ **Script ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß!**

---

## PART 5: ‡∏£‡∏±‡∏ô Training (Background)

### ‚úÖ STEP 12: ‡∏£‡∏±‡∏ô Training ‡πÉ‡∏ô Background

**‡πÉ‡∏ô Terminal (venv activated, ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà /storage/ML-number):**

```bash
# Make script executable
chmod +x train_terminal.py

# Run in background with nohup
nohup python train_terminal.py > training_output.log 2>&1 &

# ‡πÑ‡∏î‡πâ Process ID (PID) ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤ ‡πÄ‡∏ä‡πà‡∏ô:
# [1] 12345
```

**‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢:**
- `nohup` = ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡πÅ‡∏°‡πâ‡∏õ‡∏¥‡∏î terminal
- `> training_output.log` = ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å output ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå
- `2>&1` = ‡∏£‡∏ß‡∏° stderr ‡∏Å‡∏±‡∏ö stdout
- `&` = ‡∏£‡∏±‡∏ô‡πÉ‡∏ô background

‚úÖ **Training ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏•‡πâ‡∏ß! ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏î‡πâ!**

---

### ‚úÖ STEP 13: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà

**‡πÄ‡∏ä‡πá‡∏Ñ process:**

```bash
ps aux | grep train_terminal
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
user   12345  5.2 12.3  8123456 2048000 ?  R  10:30  1:23 python train_terminal.py
```

**‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ GPU ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**

```bash
nvidia-smi
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô GPU Utilization > 0%:**
```
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     12345      C   python                                     3456MiB |
+-----------------------------------------------------------------------------+
```

‚úÖ **Training ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥!**

---

## PART 6: Monitor Progress

### ‚úÖ STEP 14: ‡∏î‡∏π Log ‡πÅ‡∏ö‡∏ö Real-time

**Option 1: tail -f (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)**

```bash
tail -f training_output.log
```

**‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:**
```
2025-10-08 10:30:15 - INFO - ================================================================================
2025-10-08 10:30:15 - INFO - üöÄ PAPERSPACE ML TRAINING - TERMINAL MODE
2025-10-08 10:30:15 - INFO - ================================================================================
2025-10-08 10:30:16 - INFO - ‚úÖ GPU Available: Quadro M4000
2025-10-08 10:30:17 - INFO - ‚úÖ Data loaded: 6112 rows
2025-10-08 10:30:20 - INFO - ‚úÖ Features created: 256 features, 6112 samples
2025-10-08 10:32:15 - INFO - üî¨ Optimizing XGBoost (100 trials)...
2025-10-08 10:32:20 - INFO - [0/100] Trial 0: R¬≤ = 0.8245
2025-10-08 10:32:25 - INFO - [1/100] Trial 1: R¬≤ = 0.8512
...
```

**‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î‡∏î‡∏π (training ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠)**

---

**Option 2: less (scroll ‡πÑ‡∏î‡πâ)**

```bash
less +F training_output.log
```

**‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î scroll ‚Üí ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ scroll ‡∏Ç‡∏∂‡πâ‡∏ô-‡∏•‡∏á‡πÑ‡∏î‡πâ**
**‡∏Å‡∏î Shift+F ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏•‡∏±‡∏ö follow mode**
**‡∏Å‡∏î Q ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å**

---

**Option 3: head/tail (‡∏î‡∏π‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô)**

```bash
# ‡∏î‡∏π 50 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å
head -50 training_output.log

# ‡∏î‡∏π 50 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
tail -50 training_output.log

# ‡∏î‡∏π 100 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
tail -100 training_output.log
```

---

### ‚úÖ STEP 15: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∑‡∏ö‡∏´‡∏ô‡πâ‡∏≤

**‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏ñ‡∏∂‡∏á Trial ‡πÑ‡∏´‡∏ô‡πÅ‡∏•‡πâ‡∏ß:**

```bash
grep -oP '\[\d+/\d+\]' training_output.log | tail -10
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
[45/100]
[46/100]
[47/100]
...
```

---

**‡πÄ‡∏ä‡πá‡∏Ñ R¬≤ score ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î:**

```bash
grep "R¬≤ =" training_output.log | tail -10
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
Trial 45: R¬≤ = 0.8934
Trial 46: R¬≤ = 0.9012
Trial 47: R¬≤ = 0.8987
...
```

---

**‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÑ‡∏õ:**

```bash
grep "Training Time" training_output.log
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:**
```
Training Time: 3.45 hours
```

---

### ‚úÖ STEP 16: Monitor GPU Usage

**‡πÄ‡∏ä‡πá‡∏Ñ GPU Utilization:**

```bash
watch -n 5 nvidia-smi
```

**‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ó‡∏∏‡∏Å 5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ**
**‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î**

---

### ‚úÖ STEP 17: Estimate Time Remaining

**Timeline ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:**

```
XGBoost optimization:    2.5-3.5 hours (100 trials)
LightGBM optimization:   3-4 hours (100 trials)
CatBoost optimization:   1.5-2 hours (100 trials)
RandomForest:            1-1.5 hours (100 trials)
Ensemble:                15-30 minutes
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:                   9-12 hours
```

**‡∏î‡∏π‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÑ‡∏´‡∏ô:**

```bash
grep "Optimizing" training_output.log | tail -1
```

---

## PART 7: Download Results

### ‚úÖ STEP 18: ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ Training ‡πÄ‡∏™‡∏£‡πá‡∏à

**‡πÄ‡∏ä‡πá‡∏Ñ log:**

```bash
tail -20 training_output.log
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
‚úÖ TRAINING COMPLETE!
================================================================================
‚è±Ô∏è  Training Time: 10.23 hours
üèÜ Best Model: XGBoost
üìä Best R¬≤: 0.9234
üìâ MAE: 0.0234
üìâ RMSE: 0.0456
================================================================================
‚è±Ô∏è  Total Time: 10.45 hours
‚úÖ All done! Models saved to models/ directory
```

‚úÖ **Training ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**

---

### ‚úÖ STEP 19: ‡πÄ‡∏ä‡πá‡∏Ñ Models

```bash
ls -lh models/deployed/
```

**‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:**
```
total 50M
-rw-r--r-- 1 user user  25M Oct  8 20:30 best_model.pkl
-rw-r--r-- 1 user user  15M Oct  8 20:30 xgboost_model.pkl
-rw-r--r-- 1 user user  10M Oct  8 20:30 ensemble_model.pkl
...
```

---

### ‚úÖ STEP 20: Download Files (Jupyter UI)

**‡πÉ‡∏ô Jupyter File Browser:**

```
1. Navigate to ML-number/models/deployed/
2. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏Ç‡∏ß‡∏≤‡∏ó‡∏µ‡πà best_model.pkl
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Download"
4. ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏∞‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
```

**Download ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**
- `models/deployed/best_model.pkl` (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
- `training_output.log` (log ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
- `logs/training_*.log` (detailed log)
- `results/` (‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)

‚úÖ **‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**

---

## üéØ ‡∏™‡∏£‡∏∏‡∏õ Timeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Full ML Training Timeline         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  PART 1-3: Setup (10-15 ‡∏ô‡∏≤‡∏ó‡∏µ)              ‚îÇ
‚îÇ  ‚îú‚îÄ Clone repo (2 min)                      ‚îÇ
‚îÇ  ‚îú‚îÄ Install deps (3-5 min)                  ‚îÇ
‚îÇ  ‚îî‚îÄ Upload data (2 min)                     ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  PART 4: Create script (5 ‡∏ô‡∏≤‡∏ó‡∏µ)            ‚îÇ
‚îÇ  ‚îú‚îÄ Create file (1 min)                     ‚îÇ
‚îÇ  ‚îî‚îÄ Paste code (1 min)                      ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  PART 5: Start training (1 ‡∏ô‡∏≤‡∏ó‡∏µ)           ‚îÇ
‚îÇ  ‚îî‚îÄ nohup python train.py &                 ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚Üí ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏î‡πâ! üéâ                     ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  PART 6: Training (9-12 ‡∏ä‡∏°.)               ‚îÇ
‚îÇ  ‚îú‚îÄ XGBoost (2.5-3.5 hr)                    ‚îÇ
‚îÇ  ‚îú‚îÄ LightGBM (3-4 hr)                       ‚îÇ
‚îÇ  ‚îú‚îÄ CatBoost (1.5-2 hr)                     ‚îÇ
‚îÇ  ‚îú‚îÄ RandomForest (1-1.5 hr)                 ‚îÇ
‚îÇ  ‚îî‚îÄ Ensemble (15-30 min)                    ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚Üí ‡πÄ‡∏ä‡πá‡∏Ñ‡πÑ‡∏î‡πâ‡∏ï‡∏≠‡∏ô‡πÑ‡∏´‡∏ô‡∏Å‡πá‡πÑ‡∏î‡πâ (tail -f log)        ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  PART 7: Download (5 ‡∏ô‡∏≤‡∏ó‡∏µ)                 ‚îÇ
‚îÇ  ‚îî‚îÄ Download models + logs                  ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Total hands-on time: ~20-30 ‡∏ô‡∏≤‡∏ó‡∏µ
Total training time: 9-12 ‡∏ä‡∏°. (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏π‡πÅ‡∏•!)
```

---

## üõ†Ô∏è Troubleshooting

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 1: Training ‡πÑ‡∏°‡πà‡∏£‡∏±‡∏ô

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:** ‡∏£‡∏±‡∏ô `nohup python train_terminal.py &` ‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô process

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ error ‡πÉ‡∏ô log
cat training_output.log

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ venv active
which python
# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô: /storage/ML-number/.venv/bin/python

# ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ‚Üí activate ‡πÉ‡∏´‡∏°‡πà
source .venv/bin/activate
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 2: GPU ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:** nvidia-smi ‡πÄ‡∏´‡πá‡∏ô GPU ‡πÅ‡∏ï‡πà training ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ PyTorch CUDA
python -c "import torch; print(torch.cuda.is_available())"

# ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô False ‚Üí ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch ‡πÉ‡∏´‡∏°‡πà
pip uninstall torch -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 3: Process ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏≠‡∏á

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:** ‡πÄ‡∏ä‡πá‡∏Ñ `ps aux | grep train` ‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô process

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
```bash
# ‡∏î‡∏π log ‡∏´‡∏≤ error
tail -100 training_output.log

# ‡πÄ‡∏ä‡πá‡∏Ñ exit code
echo $?

# ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô OOM (Out of Memory)
# ‚Üí ‡∏•‡∏î N_TRIALS ‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏î features
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 4: ‡∏õ‡∏¥‡∏î Terminal ‡πÅ‡∏•‡πâ‡∏ß process ‡∏´‡∏¢‡∏∏‡∏î

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:** ‡∏õ‡∏¥‡∏î Terminal ‚Üí training ‡∏´‡∏¢‡∏∏‡∏î

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
```bash
# ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ nohup!
nohup python train_terminal.py > training_output.log 2>&1 &

# ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà:
python train_terminal.py  # ‚Üê ‡∏à‡∏∞‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏õ‡∏¥‡∏î terminal
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 5: ‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô log ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:** tail -f ‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
```bash
# Python ‡∏≠‡∏≤‡∏à buffer output
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ process ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏°‡∏±‡πâ‡∏¢
ps aux | grep train_terminal

# ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô ‚Üí ‡∏•‡∏≠‡∏á cat
cat training_output.log

# ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ -u flag (unbuffered)
nohup python -u train_terminal.py > training_output.log 2>&1 &
```

---

## üìä GPU Performance Comparison

| GPU Model | Training Time | Cost | Availability |
|-----------|---------------|------|--------------|
| **RTX 5000 Ada** | 7-8 hours | Growth Plan | High |
| **RTX A4000** | 9-10 hours | Growth Plan | High |
| **RTX P5000** | 9-10 hours | Growth Plan | Medium |
| **M4000** (Free) | 12-15 hours | Free | High |

**üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:**
- **RTX A4000**: ‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Growth Plan (‡∏£‡∏≤‡∏Ñ‡∏≤/‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û)
- **M4000** (Free): ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ~30-40%

---

## üéØ Next Steps

**‡∏´‡∏•‡∏±‡∏á Training ‡πÄ‡∏™‡∏£‡πá‡∏à:**

1. **‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•:**
```bash
python scripts/predict_single.py 0899999999
```

2. **Batch Prediction:**
```bash
python scripts/batch_predict.py input.csv
```

3. **Deploy API:**
```bash
python main.py --deploy --api-type fastapi --port 8000
```

---

## ‚úÖ Checklist

**Setup (PART 1-3):**
- [ ] GPU detected (`nvidia-smi`)
- [ ] Repository cloned (`/storage/ML-number`)
- [ ] Virtual environment created (`.venv`)
- [ ] Dependencies installed (`pip install -r requirements.txt`)
- [ ] Data uploaded (`data/raw/numberdata.csv`)
- [ ] Imports working (`from src.config import BASE_PATH`)

**Training (PART 4-5):**
- [ ] Script created (`train_terminal.py`)
- [ ] Training started (`nohup python train_terminal.py &`)
- [ ] Process running (`ps aux | grep train`)
- [ ] GPU active (`nvidia-smi`)
- [ ] Log updating (`tail -f training_output.log`)

**Monitoring (PART 6):**
- [ ] Progress visible (`grep Trial training_output.log`)
- [ ] GPU utilized (>50% in `nvidia-smi`)
- [ ] No errors in log

**Completion (PART 7):**
- [ ] Training completed (log shows "COMPLETE")
- [ ] Models saved (`models/deployed/best_model.pkl`)
- [ ] Results downloaded
- [ ] R¬≤ > 0.90 ‚úì

---

## üéì Advanced Tips

### Tip 1: Screen/Tmux (Better than nohup)

**‡πÉ‡∏ä‡πâ `screen` ‡∏´‡∏£‡∏∑‡∏≠ `tmux` ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö session management:**

```bash
# Install screen
apt-get install screen

# Start screen session
screen -S ml_training

# Run training (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ nohup)
python train_terminal.py

# Detach: Ctrl+A, D
# ‚Üí ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏¥‡∏î browser ‡πÑ‡∏î‡πâ

# Reattach ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á
screen -r ml_training
```

---

### Tip 2: Checkpoint Resume

**‡∏ñ‡πâ‡∏≤ training ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏±‡∏ô:**

```bash
# Script ‡∏°‡∏µ checkpoint manager
# ‡πÅ‡∏Å‡πâ train_terminal.py ‡πÄ‡∏û‡∏¥‡πà‡∏°:
results = train_production_pipeline(
    ...,
    resume_from_checkpoint=True  # ‚Üê ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ
)
```

---

### Tip 3: Reduce Trials for Testing

**‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏£‡∏±‡∏ô‡∏à‡∏£‡∏¥‡∏á:**

```python
# ‡πÉ‡∏ô train_terminal.py ‡πÅ‡∏Å‡πâ:
n_trials=10,  # ‚Üê ‡∏•‡∏î‡∏à‡∏≤‡∏Å 100 ‡πÄ‡∏õ‡πá‡∏ô 10
```

**‡∏à‡∏∞‡πÄ‡∏™‡∏£‡πá‡∏à‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 1-2 ‡∏ä‡∏°.** (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö)

---

## üìö ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏≠‡∏∑‡πà‡∏ô‡πÜ

- **PAPERSPACE_QUICK_START.md**: Setup ‡πÅ‡∏ö‡∏ö Notebook (interactive)
- **KAGGLE_SETUP.md**: Train ‡∏ö‡∏ô Kaggle P100 GPU
- **README.md**: User-facing documentation
- **NEXT_SESSION.md**: Session progress tracking

---

**Created**: 2025-10-08
**Version**: 1.0
**Author**: Claude Code
**Status**: ‚úÖ Ready to use

---

**üéâ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡πâ‡∏ß! ‡∏Ç‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏ä‡∏Ñ‡∏î‡∏µ!**
