# üöÄ Paperspace Modular Training - ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå

**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ML Phone Number Prediction (‡πÅ‡∏¢‡∏Å‡πÄ‡∏ó‡∏£‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•)
**Platform:** Paperspace Terminal (Free GPU M4000 ‡∏´‡∏£‡∏∑‡∏≠ Growth Plan)
**Last Updated:** 2025-10-08

---

## üìã ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Paperspace Account](#1-‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°-paperspace-account)
2. [‡∏™‡∏£‡πâ‡∏≤‡∏á Notebook ‡πÉ‡∏´‡∏°‡πà](#2-‡∏™‡∏£‡πâ‡∏≤‡∏á-notebook-‡πÉ‡∏´‡∏°‡πà)
3. [Setup Environment](#3-setup-environment)
4. [‡∏£‡∏±‡∏ô Modular Training](#4-‡∏£‡∏±‡∏ô-modular-training-‡πÅ‡∏¢‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•)
5. [Monitor Progress](#5-monitor-progress)
6. [Troubleshooting](#6-troubleshooting)

---

## 1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Paperspace Account

### ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Plan:

| Plan | GPU | Cost | Training Time | ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ |
|------|-----|------|---------------|-------|
| **Free** | M4000 (8 GB) | ‡∏ü‡∏£‡∏µ | 11-14 ‡∏ä‡∏°. | ‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ ‡∏ä‡πâ‡∏≤‡∏´‡∏ô‡πà‡∏≠‡∏¢ |
| **Growth** | A4000/P5000 (16 GB) | $8/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô | 8-11 ‡∏ä‡∏°. | ‚úÖ ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ |

### ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô:

```
1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà: https://www.paperspace.com/
2. ‡∏™‡∏°‡∏±‡∏Ñ‡∏£ (Email + Password)
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Plan:
   - Free: M4000 GPU (‡∏ü‡∏£‡∏µ)
   - Growth: A4000 GPU ($8/‡πÄ‡∏î‡∏∑‡∏≠‡∏ô - ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡πá‡∏ß)
```

---

## 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á Notebook ‡πÉ‡∏´‡∏°‡πà

### 2.1 Create Notebook

```
1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Gradient ‚Üí Notebooks
2. ‡∏Ñ‡∏•‡∏¥‡∏Å "Create"
3. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Template: "PyTorch" ‡∏´‡∏£‡∏∑‡∏≠ "Python 3"
4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å GPU:
   - Free: Free-GPU (M4000) ‚Üê ‡∏ü‡∏£‡∏µ
   - Growth: RTX A4000 ‚Üê ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤
5. ‡∏Ñ‡∏•‡∏¥‡∏Å "Start Notebook"
```

### 2.2 ‡πÄ‡∏õ‡∏¥‡∏î Terminal

```
1. ‡∏£‡∏≠ Notebook start (1-2 ‡∏ô‡∏≤‡∏ó‡∏µ)
2. ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡πÑ‡∏õ‡∏ó‡∏µ‡πà "Terminal" tab (‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á)
   ‡∏´‡∏£‡∏∑‡∏≠: Menu ‚Üí View ‚Üí Terminal
3. ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô prompt: root@xxxxxxxx:/notebooks#
```

**‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô!**

---

## 3. Setup Environment

### 3.1 Clone ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå

```bash
# ‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå /storage (persistent storage)
cd /storage

# Clone repository
git clone https://github.com/Useforclaude/ML-number.git

# ‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå
cd ML-number

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
pwd
# Output: /storage/ML-number
```

### 3.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á Virtual Environment

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á venv
python3 -m venv .venv

# Activate
source .venv/bin/activate

# ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô (.venv) ‡πÉ‡∏ô prompt:
# (.venv) root@xxxxxxxx:/storage/ML-number#
```

### 3.3 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies

```bash
# ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á packages ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
pip install -r requirements.txt

# ‡∏£‡∏≠ 5-10 ‡∏ô‡∏≤‡∏ó‡∏µ (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß internet)
```

### 3.4 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ GPU
nvidia-smi

# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 525.x.xx    Driver Version: 525.x.xx    CUDA Version: 12.0     |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
# |===============================+======================+======================|
# |   0  Quadro M4000        Off  | 00000000:00:05.0 Off |                  N/A |
# | 46%   32C    P8    11W / 120W |      0MiB /  8192MiB |      0%      Default |
# +-----------------------------------------------------------------------------+

# ‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡πá‡∏ô Quadro M4000 (Free) ‡∏´‡∏£‡∏∑‡∏≠ RTX A4000 (Growth) ‚Üí ‚úÖ OK!
```

### 3.5 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö logs

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå logs
mkdir -p logs

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
ls -la | grep logs
# drwxr-xr-x  2 root root 4096 Oct  8 10:00 logs
```

**‚úÖ Setup ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**

---

## 4. ‡∏£‡∏±‡∏ô Modular Training (‡πÅ‡∏¢‡∏Å‡∏ó‡∏µ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•)

### 4.1 ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° Pipeline

```
Total Training Time: 8-14 hours (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö GPU)
‚îú‚îÄ‚îÄ XGBoost:     2-4 hours  ‚Üí models/checkpoints/xgboost_checkpoint.pkl
‚îú‚îÄ‚îÄ LightGBM:    3-5 hours  ‚Üí models/checkpoints/lightgbm_checkpoint.pkl
‚îú‚îÄ‚îÄ CatBoost:    1-3 hours  ‚Üí models/checkpoints/catboost_checkpoint.pkl
‚îú‚îÄ‚îÄ RandomForest: 1-2 hours  ‚Üí models/checkpoints/random_forest_checkpoint.pkl
‚îî‚îÄ‚îÄ Ensemble:    15-30 min  ‚Üí models/deployed/best_model.pkl
```

### 4.2 ‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)

#### Step 1: XGBoost (2-4 hours)

```bash
# ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö background (‡πÉ‡∏ä‡πâ nohup)
nohup python train_xgboost_only.py > logs/xgb.log 2>&1 &

# ‡∏à‡∏∞‡πÑ‡∏î‡πâ PID (‡πÄ‡∏ä‡πà‡∏ô [1] 12345)
# ‡∏à‡∏î PID ‡πÑ‡∏ß‡πâ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ

# Monitor progress
tail -f logs/xgb.log

# ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:
# 2025-10-08 10:00:00 - INFO - Starting XGBoost training...
# 2025-10-08 10:00:05 - INFO - Loading data...
# 2025-10-08 10:00:10 - INFO - Data loaded: 6,100 samples
# 2025-10-08 10:00:15 - INFO - Feature engineering...
# ...
# Trial 001: R¬≤ = 0.8456
# Trial 010: R¬≤ = 0.8723
# ...

# ‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î monitor (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠!)
```

**‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à (~2-4 ‡∏ä‡∏°.)** ‡∏Å‡πà‡∏≠‡∏ô‡πÑ‡∏õ Step ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á:**
```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ process
ps aux | grep train_xgboost

# ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ ‚Üí ‡∏¢‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà
# ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‚Üí ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß

# ‡πÄ‡∏ä‡πá‡∏Ñ checkpoint file
ls -lh models/checkpoints/xgboost_checkpoint.pkl
# ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß!
```

---

#### Step 2: LightGBM (3-5 hours)

```bash
# ‡∏£‡∏≠ XGBoost ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô!

# ‡∏£‡∏±‡∏ô LightGBM
nohup python train_lightgbm_only.py > logs/lgb.log 2>&1 &

# Monitor
tail -f logs/lgb.log

# ‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à (~3-5 ‡∏ä‡∏°.)
```

**‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à:**
```bash
ls -lh models/checkpoints/lightgbm_checkpoint.pkl
```

---

#### Step 3: CatBoost (1-3 hours)

```bash
# ‡∏£‡∏±‡∏ô CatBoost
nohup python train_catboost_only.py > logs/cat.log 2>&1 &

# Monitor
tail -f logs/cat.log

# ‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à (~1-3 ‡∏ä‡∏°.)
```

**‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à:**
```bash
ls -lh models/checkpoints/catboost_checkpoint.pkl
```

---

#### Step 4: RandomForest (1-2 hours)

```bash
# ‡∏£‡∏±‡∏ô RandomForest
nohup python train_rf_only.py > logs/rf.log 2>&1 &

# Monitor
tail -f logs/rf.log

# ‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à (~1-2 ‡∏ä‡∏°.)
```

**‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏™‡∏£‡πá‡∏à:**
```bash
ls -lh models/checkpoints/random_forest_checkpoint.pkl
```

---

#### Step 5: Ensemble (15-30 minutes)

```bash
# ‡∏£‡∏≠‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô! (XGB, LGB, CAT, RF)

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏Ñ‡∏£‡∏ö 4 checkpoints
ls -lh models/checkpoints/
# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:
# xgboost_checkpoint.pkl
# lightgbm_checkpoint.pkl
# catboost_checkpoint.pkl
# random_forest_checkpoint.pkl

# ‡∏£‡∏±‡∏ô Ensemble (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á nohup ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡πá‡∏ß)
python train_ensemble_only.py

# ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:
# Loading checkpoints...
# ‚úÖ Loaded XGBoost (R¬≤ = 0.92)
# ‚úÖ Loaded LightGBM (R¬≤ = 0.89)
# ‚úÖ Loaded CatBoost (R¬≤ = 0.87)
# ‚úÖ Loaded RandomForest (R¬≤ = 0.84)
# Creating ensembles...
# Best model: Stacking Ensemble (R¬≤ = 0.93)
# Saved to: models/deployed/best_model.pkl
```

**‚úÖ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**

---

### 4.3 ‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡∏£‡∏ß‡∏î (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏•‡∏±‡∏ß timeout)

```bash
# ‡∏£‡∏±‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô (‡πÑ‡∏°‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
nohup python train_xgboost_only.py > logs/xgb.log 2>&1 &
nohup python train_lightgbm_only.py > logs/lgb.log 2>&1 &
nohup python train_catboost_only.py > logs/cat.log 2>&1 &
nohup python train_rf_only.py > logs/rf.log 2>&1 &

# Monitor all
tail -f logs/xgb.log logs/lgb.log logs/cat.log logs/rf.log

# ‡∏£‡∏≠‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à ‚Üí ‡∏£‡∏±‡∏ô ensemble
python train_ensemble_only.py
```

**‚ö†Ô∏è Pros:** ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ (‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô)
**‚ö†Ô∏è Cons:** ‡πÉ‡∏ä‡πâ RAM/GPU ‡πÄ‡∏¢‡∏≠‡∏∞ ‡∏≠‡∏≤‡∏à‡∏´‡∏°‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£

---

## 5. Monitor Progress

### 5.1 ‡πÄ‡∏ä‡πá‡∏Ñ Logs ‡πÅ‡∏ö‡∏ö Real-time

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ log ‡∏ó‡∏µ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå
tail -f logs/xgb.log

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
tail -f logs/xgb.log logs/lgb.log

# ‡πÄ‡∏ä‡πá‡∏Ñ 50 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
tail -50 logs/xgb.log

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ error ‡∏°‡∏±‡πâ‡∏¢
grep -i error logs/xgb.log
```

### 5.2 ‡πÄ‡∏ä‡πá‡∏Ñ GPU Usage

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
nvidia-smi

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ó‡∏∏‡∏Å 2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (real-time)
watch -n 2 nvidia-smi

# ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:
# +-----------------------------------------------------------------------------+
# |   0  Quadro M4000        Off  | 00000000:00:05.0 Off |                  N/A |
# | 85%   72C    P0   112W / 120W |   6800MiB /  8192MiB |     95%      Default |
# +-----------------------------------------------------------------------------+
#                                  ‚Üë GPU Usage         ‚Üë Memory Used

# ‡∏ñ‡πâ‡∏≤ GPU-Util > 70% ‚Üí ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ GPU ‚úÖ
# ‡∏ñ‡πâ‡∏≤ GPU-Util < 10% ‚Üí ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ GPU (‡∏≠‡∏≤‡∏à‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤)

# ‡∏Å‡∏î Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î
```

### 5.3 ‡πÄ‡∏ä‡πá‡∏Ñ Running Processes

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏´‡∏ô‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô
ps aux | grep train_

# Output:
# root  12345  98.5  15.2  python train_xgboost_only.py
# root  12346  85.2  12.1  python train_lightgbm_only.py

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ process ‡∏Å‡∏µ‡πà‡∏ï‡∏±‡∏ß
ps aux | grep train_ | wc -l
# Output: 2 (‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô 2 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô)
```

### 5.4 ‡πÄ‡∏ä‡πá‡∏Ñ Checkpoints

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏´‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
ls -lh models/checkpoints/

# Output:
# -rw-r--r-- 1 root root 245M Oct  8 12:34 xgboost_checkpoint.pkl
# -rw-r--r-- 1 root root 312M Oct  8 15:12 lightgbm_checkpoint.pkl
# -rw-r--r-- 1 root root 189M Oct  8 17:45 catboost_checkpoint.pkl
# -rw-r--r-- 1 root root 156M Oct  8 19:01 random_forest_checkpoint.pkl

# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Å‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå
ls models/checkpoints/*.pkl | wc -l
# Output: 4 (‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á 4 ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡πâ‡∏ß)
```

### 5.5 ‡πÄ‡∏ä‡πá‡∏Ñ Disk Space

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏á
df -h /storage

# Output:
# Filesystem      Size  Used Avail Use% Mounted on
# /dev/sda1        50G   5G   45G  10% /storage

# ‡∏ñ‡πâ‡∏≤ Use% > 90% ‚Üí ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏¢
```

---

## 6. Troubleshooting

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 1: Process ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (Killed)

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:**
```bash
tail -f logs/xgb.log
# ...
# Trial 050: R¬≤ = 0.8923
# Killed
```

**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:** ‡∏´‡∏°‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ (RAM/GPU)

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏±‡∏ô‡∏Å‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
ps aux | grep train_

# 2. ‡∏ñ‡πâ‡∏≤‡∏£‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô ‚Üí ‡∏´‡∏¢‡∏∏‡∏î 1-2 ‡∏ï‡∏±‡∏ß
kill <PID>

# 3. ‡∏£‡∏±‡∏ô‡∏ó‡∏µ‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ó‡∏ô (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 2: GPU ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (0% usage)

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:**
```bash
nvidia-smi
# GPU-Util: 0%
```

**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:** ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ GPU

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# 1. ‡πÄ‡∏ä‡πá‡∏Ñ log ‡∏ß‡πà‡∏≤‡∏°‡∏µ warning ‡∏°‡∏±‡πâ‡∏¢
grep -i "gpu" logs/xgb.log

# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ XGBoost detect GPU ‡∏°‡∏±‡πâ‡∏¢
python -c "import xgboost; print(xgboost.__version__)"

# 3. ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‚Üí ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á xgboost ‡πÉ‡∏´‡∏°‡πà
pip install --upgrade xgboost
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 3: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå train_xgboost_only.py

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:**
```bash
python train_xgboost_only.py
# Error: No such file or directory
```

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
pwd
# Output: /storage/ML-number

# ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ‚Üí cd ‡∏Å‡∏•‡∏±‡∏ö
cd /storage/ML-number

# 2. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏±‡πâ‡∏¢
ls train_*.py
# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô:
# train_xgboost_only.py
# train_lightgbm_only.py
# ...

# 3. ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ ‚Üí git pull ‡πÉ‡∏´‡∏°‡πà
git pull origin main
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 4: ModuleNotFoundError

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:**
```bash
python train_xgboost_only.py
# ModuleNotFoundError: No module named 'xgboost'
```

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# 1. ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤ activate venv ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏∂‡∏¢‡∏±‡∏á
which python
# Output: /storage/ML-number/.venv/bin/python ‚Üê ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
# Output: /usr/bin/python ‚Üê ‚ùå ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà activate

# 2. ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà activate ‚Üí activate
source .venv/bin/activate

# 3. ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á error ‚Üí ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡∏°‡πà
pip install -r requirements.txt
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 5: Session timeout / Browser ‡∏õ‡∏¥‡∏î

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:** ‡∏õ‡∏¥‡∏î browser ‚Üí ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà ‚Üí ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏õ‡∏ñ‡∏∂‡∏á‡πÑ‡∏´‡∏ô

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# 1. ‡πÄ‡∏ä‡πá‡∏Ñ process ‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà
ps aux | grep train_

# 2. ‡πÄ‡∏ä‡πá‡∏Ñ log ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
tail -50 logs/xgb.log

# 3. ‡πÄ‡∏ä‡πá‡∏Ñ checkpoint
ls -lh models/checkpoints/

# ‚úÖ ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ nohup ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏ï‡πà‡∏≠‡πÅ‡∏°‡πâ‡∏õ‡∏¥‡∏î browser!
```

---

### ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ 6: Training ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:** XGBoost ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤ >5 ‡∏ä‡∏°. (‡∏õ‡∏Å‡∏ï‡∏¥ 2-3 ‡∏ä‡∏°.)

**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:**
1. ‡πÉ‡∏ä‡πâ Free Plan (M4000) ‚Üí ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ Growth Plan (A4000)
2. GPU ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡∏á‡∏≤‡∏ô (‡πÉ‡∏ä‡πâ CPU ‡πÅ‡∏ó‡∏ô)

**‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç:**
```bash
# 1. ‡πÄ‡∏ä‡πá‡∏Ñ GPU usage
nvidia-smi
# ‡∏ñ‡πâ‡∏≤ GPU-Util < 10% ‚Üí ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ GPU

# 2. ‡πÄ‡∏ä‡πá‡∏Ñ log
grep -i "device" logs/xgb.log
# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô: Device: cuda
# ‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡πá‡∏ô: Device: cpu ‚Üí ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤

# 3. ‡∏ñ‡πâ‡∏≤‡∏ä‡πâ‡∏≤‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ GPU ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡∏õ‡∏Å‡∏ï‡∏¥ (M4000 ‡∏ä‡πâ‡∏≤)
# ‡∏´‡∏£‡∏∑‡∏≠ ‚Üí ‡∏≠‡∏±‡∏û‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏õ‡πá‡∏ô Growth Plan (A4000)
```

---

## 7. Expected Results

### ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•:

| GPU | XGBoost | LightGBM | CatBoost | RandomForest | Ensemble | Total |
|-----|---------|----------|----------|--------------|----------|-------|
| **M4000** (Free) | 3-4 ‡∏ä‡∏°. | 4-5 ‡∏ä‡∏°. | 2-3 ‡∏ä‡∏°. | 1.5 ‡∏ä‡∏°. | 30 ‡∏ô‡∏≤‡∏ó‡∏µ | **11-14 ‡∏ä‡∏°.** |
| **A4000** (Growth) | 2-3 ‡∏ä‡∏°. | 3-4 ‡∏ä‡∏°. | 1-2 ‡∏ä‡∏°. | 1 ‡∏ä‡∏°. | 15 ‡∏ô‡∏≤‡∏ó‡∏µ | **8-11 ‡∏ä‡∏°.** |

### R¬≤ Scores:

| Model | Expected R¬≤ |
|-------|-------------|
| XGBoost | 0.88-0.92 |
| LightGBM | 0.86-0.90 |
| CatBoost | 0.85-0.89 |
| RandomForest | 0.82-0.86 |
| **Ensemble** | **0.90-0.93** ‚úÖ |

---

## 8. Quick Commands Reference

```bash
# === SETUP ===
cd /storage
git clone https://github.com/Useforclaude/ML-number.git
cd ML-number
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
mkdir -p logs

# === TRAINING ===
nohup python train_xgboost_only.py > logs/xgb.log 2>&1 &
nohup python train_lightgbm_only.py > logs/lgb.log 2>&1 &
nohup python train_catboost_only.py > logs/cat.log 2>&1 &
nohup python train_rf_only.py > logs/rf.log 2>&1 &
python train_ensemble_only.py

# === MONITORING ===
tail -f logs/xgb.log              # Monitor log
watch -n 2 nvidia-smi             # Monitor GPU
ps aux | grep train_              # Check processes
ls -lh models/checkpoints/        # Check checkpoints

# === TROUBLESHOOTING ===
kill <PID>                        # Stop process
grep -i error logs/xgb.log        # Find errors
df -h /storage                    # Check disk space
```

---

## 9. After Training

### 9.1 ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Best Model

```bash
# ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ best model
ls -lh models/deployed/best_model.pkl

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡πà‡∏≤‡∏ô Jupyter File Browser:
# 1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà File Browser (‡∏î‡πâ‡∏≤‡∏ô‡∏ã‡πâ‡∏≤‡∏¢)
# 2. Navigate: /storage/ML-number/models/deployed/
# 3. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏Ç‡∏ß‡∏≤ best_model.pkl ‚Üí Download
```

### 9.2 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Model

```python
# ‡πÉ‡∏ô Python shell ‡∏´‡∏£‡∏∑‡∏≠ Jupyter notebook
import joblib

# Load model
model_pkg = joblib.load('models/deployed/best_model.pkl')

print(f"Model: {model_pkg['model_name']}")
print(f"R¬≤ Score: {model_pkg['metrics']['test_r2']:.4f}")
print(f"MAE: {model_pkg['metrics']['test_mae']:.2f}")

# Test prediction
phone = "0899999999"
# ... (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° features ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏≠‡∏ô training)
```

---

## 10. Summary Checklist

**‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°:**
- [ ] ‡∏™‡∏°‡∏±‡∏Ñ‡∏£ Paperspace account
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á Notebook (Free GPU ‡∏´‡∏£‡∏∑‡∏≠ Growth)
- [ ] ‡πÄ‡∏õ‡∏¥‡∏î Terminal

**Setup:**
- [ ] Clone repository
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á venv + activate
- [ ] ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á requirements
- [ ] ‡πÄ‡∏ä‡πá‡∏Ñ GPU (nvidia-smi)
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå logs

**Training:**
- [ ] ‡∏£‡∏±‡∏ô XGBoost (2-4 ‡∏ä‡∏°.)
- [ ] ‡∏£‡∏±‡∏ô LightGBM (3-5 ‡∏ä‡∏°.)
- [ ] ‡∏£‡∏±‡∏ô CatBoost (1-3 ‡∏ä‡∏°.)
- [ ] ‡∏£‡∏±‡∏ô RandomForest (1-2 ‡∏ä‡∏°.)
- [ ] ‡∏£‡∏±‡∏ô Ensemble (15-30 ‡∏ô‡∏≤‡∏ó‡∏µ)

**Verification:**
- [ ] ‡πÄ‡∏ä‡πá‡∏Ñ logs ‡πÑ‡∏°‡πà‡∏°‡∏µ error
- [ ] ‡πÄ‡∏ä‡πá‡∏Ñ checkpoints ‡∏Ñ‡∏£‡∏ö 4 ‡πÑ‡∏ü‡∏•‡πå
- [ ] ‡πÄ‡∏ä‡πá‡∏Ñ best_model.pkl ‡πÉ‡∏ô deployed/
- [ ] R¬≤ > 0.90 ‚úÖ

**‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**

---

**Created:** 2025-10-08
**Session:** 012 - Paperspace Modular Training Guide
**Total Time:** 8-14 hours (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö GPU)
**Expected R¬≤:** 0.90-0.93 ‚úÖ
