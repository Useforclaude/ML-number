# ğŸ¯ NEXT SESSION GUIDE

**Last Updated**: 2025-10-11 18:15
**Session**: 014 - Paperspace Deployment Complete
**Status**: âœ… READY FOR TRAINING!

---

## âœ… SESSION 014 - PAPERSPACE DEPLOYMENT COMPLETE

### ğŸš€ What Was Done:

1. **Git Push to GitHub** âœ…
   - Commit: `40a9e65`
   - Session 013 documentation + project refactor
   - 76 files changed (+4,573 insertions, -244 deletions)

2. **Paperspace Setup** âœ…
   - Path: `/notebooks/ML-number`
   - Git cloned from: https://github.com/Useforclaude/ML-number.git
   - Virtual environment created: `.venv`
   - All requirements installed âœ…
   - Imports verified âœ…
   - BASE_PATH detected: `/storage/number-ML` âœ…

3. **Key Benefits** ğŸ¯
   - **Persistent storage**: Files in `/notebooks/` won't be deleted when session expires (unlike Colab/Kaggle)
   - **All Codex fixes included**: Tuple unpacking bug fixed in all 6 training scripts
   - **Ready to train**: Just need to upload data file

---

## ğŸ“‹ CRITICAL: Paperspace Environment Info (à¸šà¸±à¸™à¸—à¸¶à¸à¹„à¸§à¹‰!)

```bash
# Paperspace Paths (MUST REMEMBER!)
PROJECT_PATH="/notebooks/ML-number"
BASE_PATH="/storage/number-ML"  # Auto-detected by config
VENV_PATH="/notebooks/ML-number/.venv"
LOGS_PATH="/notebooks/ML-number/logs"

# Key Feature: Persistent Storage âœ…
# à¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆ /notebooks/ à¹„à¸¡à¹ˆà¹‚à¸”à¸™à¸¥à¸šà¹€à¸¡à¸·à¹ˆà¸­ session à¸«à¸¡à¸”à¹€à¸§à¸¥à¸²
# (à¸•à¹ˆà¸²à¸‡à¸ˆà¸²à¸ Colab/Kaggle à¸—à¸µà¹ˆà¸¥à¸šà¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡!)

# Git Remote
GIT_REMOTE="https://github.com/Useforclaude/ML-number.git"
```

---

## ğŸ¯ NEXT STEPS (Session 015)

### **Step 1: Upload Data File** â¬†ï¸

**Option A: Web Upload (Easiest)**
```bash
# In Paperspace:
# 1. Click "Upload" button in file browser
# 2. Navigate to local: /home/u-and-an/projects/number-ML/data/raw/numberdata.csv
# 3. Upload to Paperspace: /notebooks/ML-number/data/raw/
# 4. Verify:
(.venv) root@xxx:/notebooks/ML-number# ls -lh data/raw/numberdata.csv
# Expected: ~93 KB file
```

**Option B: SCP Upload (Alternative)**
```bash
# From local machine:
scp /home/u-and-an/projects/number-ML/data/raw/numberdata.csv \
    root@paperspace:/notebooks/ML-number/data/raw/
```

### **Step 2: Verify Setup** âœ…

```bash
# Activate venv
cd /notebooks/ML-number
source .venv/bin/activate

# Check data file exists
ls -lh data/raw/numberdata.csv
# Expected: ~93 KB, 6,112 rows

# Test data loading (CRITICAL!)
python -c "
from src.data_handler import load_and_clean_data
df_raw, df_cleaned = load_and_clean_data(filter_outliers_param=True, max_price=100000)
print(f'âœ… Data loaded: raw={len(df_raw)} rows, cleaned={len(df_cleaned)} rows')
"
# Expected output: "âœ… Data loaded: raw=6112 rows, cleaned=6100 rows"
```

### **Step 3: Start Training** ğŸš€

```bash
# Make sure you're in venv
cd /notebooks/ML-number
source .venv/bin/activate

# Run models in background (no timeout!)
nohup python training/modular/train_xgboost_only.py > logs/xgb.log 2>&1 &
nohup python training/modular/train_lightgbm_only.py > logs/lgb.log 2>&1 &
nohup python training/modular/train_catboost_only.py > logs/cat.log 2>&1 &
nohup python training/modular/train_rf_only.py > logs/rf.log 2>&1 &

# Monitor first model
tail -f logs/xgb.log

# Verify data loading (CRITICAL CHECK!)
grep "Data loaded" logs/*.log
# Expected: "âœ… Data loaded: raw=6112 rows, cleaned=6100 rows"
```

### **Step 4: Monitor Training** ğŸ‘€

```bash
# Check running processes
ps aux | grep train_

# Monitor logs
tail -f logs/xgb.log       # XGBoost progress
tail -f logs/lgb.log       # LightGBM progress
tail -f logs/cat.log       # CatBoost progress
tail -f logs/rf.log        # RandomForest progress

# Check GPU usage (if available)
watch -n 5 nvidia-smi

# Check data loading confirmation (VERIFY CODEX FIX!)
grep "Data loaded" logs/*.log
# MUST show: raw=6112, cleaned=6100
```

### **Step 5: After Training Complete** âœ…

```bash
# Check RÂ² scores
grep "RÂ²" logs/*.log
grep "Test RÂ²" logs/*.log

# Expected Results (after Codex fix):
# XGBoost:     RÂ² ~0.88-0.92 âœ…
# LightGBM:    RÂ² ~0.86-0.90 âœ…
# CatBoost:    RÂ² ~0.85-0.89 âœ…
# RandomForest: RÂ² ~0.82-0.86 âœ…

# Check checkpoints saved
ls -lh models/checkpoints/
# Expected: xgboost_checkpoint.pkl, lightgbm_checkpoint.pkl, etc.

# Run ensemble (final step)
python training/modular/train_ensemble_only.py

# Check deployed model
ls -lh models/deployed/best_model.pkl
# Expected: Best model from ensemble
```

---

## ğŸ“Š Expected Timeline

```
XGBoost:     2-3 hours  â†’ checkpoint saved
LightGBM:    3-4 hours  â†’ checkpoint saved
CatBoost:    1-2 hours  â†’ checkpoint saved
RandomForest: 1 hour    â†’ checkpoint saved
Ensemble:    15-30 min  â†’ best model deployed
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:       8-11 hours (can run in parallel!)
```

---

## ğŸ” Verification Checklist

### Before Training:
- [x] Code pushed to GitHub (commit 40a9e65) âœ…
- [x] Code cloned to Paperspace âœ…
- [x] Virtual environment created âœ…
- [x] Requirements installed âœ…
- [x] Imports verified âœ…
- [x] BASE_PATH detected (/storage/number-ML) âœ…
- [x] Logs directory created âœ…
- [ ] Data file uploaded to Paperspace
- [ ] Data loading tested (verify raw=6112, cleaned=6100)

### During Training:
- [ ] Logs show: "âœ… Data loaded: raw=6112 rows, cleaned=6100 rows" (confirms Codex fix!)
- [ ] No ValueError or TypeError
- [ ] GPU active (if available)
- [ ] Checkpoints saving to models/checkpoints/
- [ ] RÂ² improving during optimization

### After Training:
- [ ] All 4 model checkpoints saved
- [ ] RÂ² scores logged and â‰¥ 0.85
- [ ] Ensemble created successfully
- [ ] Best model deployed to models/deployed/
- [ ] Can download trained models

---

## ğŸš¨ CRITICAL: Data Loading Verification

**WHY THIS IS CRITICAL:**

Session 013 fixed a bug where training scripts would crash or use wrong data format:

```python
# âŒ BEFORE (Bug - Session 012):
df_cleaned = load_and_clean_data(filter_outliers_param=True, max_price=100000)
# Result: df_cleaned = tuple (df_raw, df_cleaned) â† WRONG!
# Impact: Training fails OR uses unfiltered data (RÂ² = 0.4)

# âœ… AFTER (Fixed - Session 013 by Codex):
df_raw, df_cleaned = load_and_clean_data(filter_outliers_param=True, max_price=100000)
logger.info(f"âœ… Data loaded: raw={len(df_raw)} rows, cleaned={len(df_cleaned)} rows")
# Result: df_cleaned = DataFrame with filtered data â† CORRECT!
# Impact: Training succeeds with RÂ² = 0.85-0.92 âœ…
```

**VERIFICATION COMMAND (Run this FIRST!):**
```bash
grep "Data loaded" logs/*.log
# MUST see: "âœ… Data loaded: raw=6112 rows, cleaned=6100 rows"
# If you see this, Codex fix is working! âœ…
# If you DON'T see this, STOP and debug!
```

---

## ğŸ¯ Success Criteria

**Training is SUCCESSFUL if:**
- âœ… Logs show: `raw=6112 rows, cleaned=6100 rows` (confirms Codex fix works!)
- âœ… No ValueError or TypeError during execution
- âœ… All 4 model checkpoints saved to models/checkpoints/
- âœ… RÂ² score â‰¥ 0.85 (target: 0.90+)
- âœ… Best model deployed to models/deployed/best_model.pkl

**Training FAILED if:**
- âŒ ValueError: "too many values to unpack" (Codex fix not applied)
- âŒ RÂ² still around 0.4 (using unfiltered data - bug not fixed)
- âŒ Missing checkpoints or process crashes
- âŒ Import errors or dependency issues

---

## ğŸ”„ Recovery Commands (If Needed)

### If Git Pull Needed:
```bash
cd /notebooks/ML-number
git fetch origin
git reset --hard origin/main
```

### If Imports Fail:
```bash
cd /notebooks/ML-number
source .venv/bin/activate
pip install -r requirements.txt --upgrade
```

### If Training Crashes:
```bash
# Check which checkpoint exists
ls -lh models/checkpoints/

# Resume from next model
# Example: If XGBoost done, continue with LightGBM
nohup python training/modular/train_lightgbm_only.py > logs/lgb.log 2>&1 &
```

### If Data File Missing:
```bash
# Re-upload from local
scp /home/u-and-an/projects/number-ML/data/raw/numberdata.csv \
    root@paperspace:/notebooks/ML-number/data/raw/
```

---

## ğŸ“š Monitoring Commands Reference

```bash
# Check running processes
ps aux | grep train_

# Monitor specific log
tail -f logs/xgb.log

# Check all logs for errors
grep -i "error" logs/*.log

# Check data loading (CRITICAL!)
grep "Data loaded" logs/*.log

# Check RÂ² progress
grep "RÂ²" logs/*.log

# Check GPU usage
watch -n 5 nvidia-smi

# Check disk space
df -h /storage

# Check memory usage
free -h

# Check checkpoints
ls -lh models/checkpoints/

# Check deployed model
ls -lh models/deployed/
```

---

## ğŸ“ Key Files & Paths (Paperspace)

### Project Structure:
```
/notebooks/ML-number/
â”œâ”€â”€ training/modular/          # Training scripts (5 files)
â”‚   â”œâ”€â”€ train_xgboost_only.py
â”‚   â”œâ”€â”€ train_lightgbm_only.py
â”‚   â”œâ”€â”€ train_catboost_only.py
â”‚   â”œâ”€â”€ train_rf_only.py
â”‚   â””â”€â”€ train_ensemble_only.py
â”œâ”€â”€ data/raw/                  # Data files
â”‚   â””â”€â”€ numberdata.csv         # Upload this!
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/           # Model checkpoints
â”‚   â””â”€â”€ deployed/              # Best model
â”œâ”€â”€ logs/                      # Training logs
â”œâ”€â”€ src/                       # Core code
â”œâ”€â”€ .venv/                     # Virtual environment (persistent!)
â””â”€â”€ requirements.txt
```

### Important Paths:
- **Project**: `/notebooks/ML-number`
- **Data**: `/notebooks/ML-number/data/raw/numberdata.csv`
- **Venv**: `/notebooks/ML-number/.venv`
- **Logs**: `/notebooks/ML-number/logs/`
- **Checkpoints**: `/notebooks/ML-number/models/checkpoints/`
- **Deployed**: `/notebooks/ML-number/models/deployed/`

---

## ğŸ¯ Session 015 Quick Start (Copy-Paste)

```bash
# === SESSION 015: START TRAINING ===

# Step 1: Navigate to project
cd /notebooks/ML-number

# Step 2: Activate venv
source .venv/bin/activate

# Step 3: Verify data file exists
ls -lh data/raw/numberdata.csv
# If missing, upload it first!

# Step 4: Test data loading (CRITICAL!)
python -c "
from src.data_handler import load_and_clean_data
df_raw, df_cleaned = load_and_clean_data(filter_outliers_param=True, max_price=100000)
print(f'âœ… Data loaded: raw={len(df_raw)} rows, cleaned={len(df_cleaned)} rows')
"
# Expected: raw=6112, cleaned=6100

# Step 5: Start training (all models in parallel)
nohup python training/modular/train_xgboost_only.py > logs/xgb.log 2>&1 &
nohup python training/modular/train_lightgbm_only.py > logs/lgb.log 2>&1 &
nohup python training/modular/train_catboost_only.py > logs/cat.log 2>&1 &
nohup python training/modular/train_rf_only.py > logs/rf.log 2>&1 &

# Step 6: Monitor
tail -f logs/xgb.log

# Step 7: Verify data loading in logs
grep "Data loaded" logs/*.log
# MUST show: raw=6112, cleaned=6100 âœ…

# === LET IT RUN FOR 8-11 HOURS ===
```

---

## ğŸ“Š Documentation References

- **Session 013 Fix**: `docs/sessions/SESSION_013_FIX.md` (if exists)
- **Codex Methodology**: `CLAUDE.md` (Case Study #5)
- **Paperspace Guide**: `docs/guides/paperspace/PAPERSPACE_START_FROM_ZERO.md`
- **Modular Training**: `docs/guides/paperspace/PAPERSPACE_MODULAR_TRAINING_GUIDE.md`

---

## âœ… Status Summary

**Session 014**: COMPLETE âœ…
- Git push to GitHub âœ…
- Paperspace setup complete âœ…
- All Codex fixes verified âœ…
- Ready for training âœ…

**Session 015**: NEXT
- Upload data file
- Start training (8-11 hours)
- Monitor progress
- Verify RÂ² â‰¥ 0.85
- Deploy best model

---

**Created**: 2025-10-11 18:15
**Session**: 014 - Paperspace Deployment Complete
**Status**: âœ… READY FOR TRAINING!
**Next**: Upload data â†’ Start training â†’ Monitor RÂ² ğŸš€

---

**à¸à¸£à¹‰à¸­à¸¡à¹€à¸—à¸£à¸™à¹à¸¥à¹‰à¸§!** à¸­à¸±à¸à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ data à¹à¸¥à¹‰à¸§à¹€à¸£à¸´à¹ˆà¸¡à¹„à¸”à¹‰à¹€à¸¥à¸¢! ğŸ¯
